{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cbb6b0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1 ‚Äî Imports & global config\n",
    "import os, json, math, warnings, itertools, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "plt.rcParams[\"figure.dpi\"] = 140\n",
    "\n",
    "# Paths\n",
    "DATA_PATH  = \"data/housing_adequacy_dataset.csv\"\n",
    "CACHE_PATH = \"best_params_arima_cache.json\"\n",
    "\n",
    "# Reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "random.seed(RANDOM_STATE)\n",
    "\n",
    "# Quarterly data\n",
    "SEASONAL_PERIOD = 4\n",
    "\n",
    "# Time split for holdout\n",
    "CUTOFF_DATE = \"2018-12-31\"   # train ‚â§ cutoff, test > cutoff\n",
    "\n",
    "# Rolling-CV controls (tuning)\n",
    "INITIAL_CV_START = \"2010-12-31\"  # start CV later to ensure decent train size\n",
    "ROLLING_STEP_FOR_TUNING = 1      # set 2 to halve folds\n",
    "FOLD_LIMIT = 6                   # only last K folds; None = all\n",
    "MIN_TRAIN_PER_FOLD = 16          # min non-NaN points required in a fold (quarters)\n",
    "PRUNE_DURING_TUNING = True       # stop a candidate early if already worse than best\n",
    "\n",
    "# Tuning grids (keep small; expand after pipeline works)\n",
    "ARIMA_GRID = {\"p\":[0,1,2], \"d\":[0,1], \"q\":[0,1,2]}\n",
    "SARIMA_GRID = {\n",
    "    \"p\":[0,1,2], \"d\":[0,1], \"q\":[0,1,2],\n",
    "    \"P\":[0,1],   \"D\":[0,1], \"Q\":[0,1], \"s\":[SEASONAL_PERIOD]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "22de3b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2 ‚Äî Data prep (regular quarterly) & splits\n",
    "\n",
    "def build_univariate_frame(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Return a regular quarterly panel with columns: province, quarter (datetime), y (float).\n",
    "    Reindexes each province to a full quarterly DatetimeIndex. Internal NaNs allowed.\n",
    "    \"\"\"\n",
    "    df = df.sort_values([\"province\", \"quarter\"]).copy()\n",
    "    df[\"quarter\"] = pd.PeriodIndex(df[\"quarter\"], freq=\"Q\").to_timestamp()\n",
    "    out = []\n",
    "    for prov, g in df.groupby(\"province\"):\n",
    "        start = g[\"quarter\"].min()\n",
    "        end   = g[\"quarter\"].max()\n",
    "        idx = pd.date_range(start, end, freq=\"Q\")\n",
    "        y = (g.set_index(\"quarter\")[\"dwelling_starts\"]\n",
    "               .astype(float)\n",
    "               .reindex(idx))  # may introduce NaNs if gaps\n",
    "        out.append(pd.DataFrame({\"province\":prov, \"quarter\":idx, \"y\":y}))\n",
    "    uni = pd.concat(out, ignore_index=True)\n",
    "    return uni\n",
    "\n",
    "def chrono_split(df, cutoff=CUTOFF_DATE):\n",
    "    cutoff = pd.Timestamp(cutoff)\n",
    "    tr = df[df[\"quarter\"] <= cutoff].copy()\n",
    "    te = df[df[\"quarter\"] >  cutoff].copy()\n",
    "    return tr, te\n",
    "\n",
    "def rolling_split(df, initial=None, step=1, fh=1):\n",
    "    \"\"\"\n",
    "    Expanding-window CV over the full panel (all provinces).\n",
    "    Yields (train_df, test_df) for the next fh quarters (panel slices).\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df[\"quarter\"] = pd.to_datetime(df[\"quarter\"])  # <- harden\n",
    "    dates = pd.Index(df[\"quarter\"].unique()).sort_values()  # robust, no mixed types\n",
    "\n",
    "    # normalize 'initial' to Timestamp\n",
    "    if initial is None:\n",
    "        initial = dates[int(0.6 * len(dates))]\n",
    "    else:\n",
    "        initial = pd.Timestamp(initial)\n",
    "\n",
    "    # use Index.get_indexer instead of np.searchsorted to avoid dtype issues\n",
    "    start_idx = dates.get_indexer([initial], method=\"ffill\")[0]\n",
    "    start_idx = max(start_idx, 0)\n",
    "\n",
    "    for i in range(start_idx, len(dates) - fh, step):\n",
    "        train_end = dates[i]\n",
    "        test_slice = dates[i+1 : i+1+fh]\n",
    "        tr = df[df[\"quarter\"] <= train_end].copy()\n",
    "        te = df[df[\"quarter\"].isin(test_slice)].copy()\n",
    "        if not te.empty:\n",
    "            yield tr, te\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f22ffc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3 ‚Äî Metrics & baselines\n",
    "\n",
    "def metrics(y_true, y_pred):\n",
    "    mae  = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    eps = 1e-8\n",
    "    smape = 100 * np.mean(2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred) + eps))\n",
    "    return {\"MAE\": mae, \"RMSE\": rmse, \"sMAPE\": smape}\n",
    "\n",
    "def mase(y_true: pd.Series, y_pred: pd.Series, y_train: pd.Series, season:int=1, eps:float=1e-12):\n",
    "    if season == 1:\n",
    "        denom = np.mean(np.abs(y_train.diff().dropna()))\n",
    "    else:\n",
    "        denom = np.mean(np.abs(y_train.diff(season).dropna()))\n",
    "    denom = max(denom if denom is not None else np.nan, eps)\n",
    "    return float(np.mean(np.abs(y_true - y_pred))) / denom\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e9a08334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4 ‚Äî ARIMA/SARIMA wrappers\n",
    "\n",
    "def fit_sarimax_endog(y_tr: pd.Series, order=(1,1,1), seasonal_order=(0,0,0,0)):\n",
    "    \"\"\"\n",
    "    y_tr: pandas Series indexed by a regular quarterly DatetimeIndex.\n",
    "    missing='drop' allows internal NaNs; we also disable strict stationarity/invertibility.\n",
    "    \"\"\"\n",
    "    model = SARIMAX(\n",
    "        endog=y_tr,\n",
    "        order=order,\n",
    "        seasonal_order=seasonal_order,\n",
    "        enforce_stationarity=False,\n",
    "        enforce_invertibility=False,\n",
    "        trend=None,\n",
    "        missing=\"drop\"   # <‚Äî key for robustness\n",
    "    )\n",
    "    res = model.fit(disp=False, method_kwargs={\"warn_convergence\": False})\n",
    "    return res\n",
    "\n",
    "def forecast_steps(res, steps=1):\n",
    "    fc = res.get_forecast(steps=steps)\n",
    "    return fc.predicted_mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "60acb7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5 ‚Äî Tuning utilities\n",
    "\n",
    "def cartesian_product(grid_dict):\n",
    "    keys = list(grid_dict.keys())\n",
    "    for values in itertools.product(*[grid_dict[k] for k in keys]):\n",
    "        yield dict(zip(keys, values))\n",
    "\n",
    "def rolling_score_series(order, seasonal_order, series_df, fh=1, season_for_mase=1,\n",
    "                         initial=INITIAL_CV_START, step=ROLLING_STEP_FOR_TUNING,\n",
    "                         fold_limit=FOLD_LIMIT, min_train=MIN_TRAIN_PER_FOLD,\n",
    "                         prune=PRUNE_DURING_TUNING, verbose=False):\n",
    "    \"\"\"\n",
    "    Score a single province series across multiple folds.\n",
    "    Returns average MASE (lower = better) or inf if no valid fold.\n",
    "    \"\"\"\n",
    "    \n",
    "    series_df = series_df.copy()\n",
    "    series_df[\"quarter\"] = pd.to_datetime(series_df[\"quarter\"])\n",
    "    \n",
    "    folds = list(rolling_split(series_df, initial=initial, step=step, fh=fh))\n",
    "    if fold_limit is not None and len(folds) > fold_limit:\n",
    "        folds = folds[-fold_limit:]\n",
    "\n",
    "    scores = []\n",
    "    first_error = None\n",
    "    for (tr_all, te_all) in folds:\n",
    "        y_tr = tr_all[\"y\"].astype(float)\n",
    "        y_te = te_all[\"y\"].astype(float)\n",
    "        if y_tr.notna().sum() < min_train:\n",
    "            continue\n",
    "        try:\n",
    "            res = fit_sarimax_endog(y_tr, order=order, seasonal_order=seasonal_order)\n",
    "            y_hat = forecast_steps(res, steps=len(y_te)).values\n",
    "            s = mase(y_te, pd.Series(y_hat, index=y_te.index), y_tr, season=season_for_mase)\n",
    "            if np.isfinite(s):\n",
    "                scores.append(s)\n",
    "        except Exception as e:\n",
    "            if first_error is None:\n",
    "                first_error = f\"{type(e).__name__}: {e}\"\n",
    "\n",
    "        # simple pruning: if growing mean already huge vs na√Øve, bail\n",
    "        if prune and scores:\n",
    "            cur = np.mean(scores)\n",
    "            if cur > 5.0:  # heuristic threshold\n",
    "                break\n",
    "\n",
    "    if not scores:\n",
    "        if verbose and first_error:\n",
    "            print(f\"   failed for order={order}, seas={seasonal_order} -> {first_error}\")\n",
    "        return np.inf\n",
    "    return float(np.mean(scores))\n",
    "\n",
    "def tune_one_series(series_df, model_type=\"arima\", fh=1, verbose=False):\n",
    "    \"\"\"\n",
    "    Returns (best_order, best_seasonal_order, best_score).\n",
    "    For ARIMA: seasonal_order=(0,0,0,0).\n",
    "    For SARIMA: seasonal_order from SARIMA_GRID.\n",
    "    \"\"\"\n",
    "    if model_type == \"arima\":\n",
    "        best = (np.inf, (0,0,0), (0,0,0,0))\n",
    "        for g in cartesian_product(ARIMA_GRID):\n",
    "            order = (g[\"p\"], g[\"d\"], g[\"q\"])\n",
    "            seas  = (0,0,0,0)\n",
    "            score = rolling_score_series(order, seas, series_df, fh=fh, season_for_mase=1, verbose=verbose)\n",
    "            if verbose: print(f\"ARIMA{order} -> {score:.3f}\")\n",
    "            if score < best[0]:\n",
    "                best = (score, order, seas)\n",
    "        return best[1], best[2], best[0]\n",
    "\n",
    "    elif model_type == \"sarima\":\n",
    "        best = (np.inf, (0,0,0), (0,0,0,SEASONAL_PERIOD))\n",
    "        for g in cartesian_product(SARIMA_GRID):\n",
    "            order = (g[\"p\"], g[\"d\"], g[\"q\"])\n",
    "            seas  = (g[\"P\"], g[\"D\"], g[\"Q\"], g[\"s\"])\n",
    "            score = rolling_score_series(order, seas, series_df, fh=fh, season_for_mase=SEASONAL_PERIOD, verbose=verbose)\n",
    "            if verbose: print(f\"SARIMA{order}x{seas} -> {score:.3f}\")\n",
    "            if score < best[0]:\n",
    "                best = (score, order, seas)\n",
    "        return best[1], best[2], best[0]\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"model_type must be 'arima' or 'sarima'\")\n",
    "\n",
    "def tune_all_provinces(train_df, models_to_run=(\"arima\",\"sarima\"), fh=1):\n",
    "    \"\"\"\n",
    "    Returns dict: {(province, model_type): {\"order\":..., \"seasonal_order\":..., \"score\":...}}\n",
    "    \"\"\"\n",
    "    best_params = {}\n",
    "    provinces = list(train_df[\"province\"].unique())\n",
    "    total = len(provinces) * len(models_to_run)\n",
    "    task = 0\n",
    "    print(f\"üßÆ Tuning {len(provinces)} provinces √ó {len(models_to_run)} models\")\n",
    "    for prov, gtr in train_df.groupby(\"province\"):\n",
    "        series = gtr[[\"quarter\",\"y\"]].reset_index(drop=True)\n",
    "        for m in models_to_run:\n",
    "            task += 1\n",
    "            print(f\"‚Üí [{task}/{total}] {prov.upper()} ‚Äî {m.upper()} ...\", end=\" \", flush=True)\n",
    "            order, seas, score = tune_one_series(series, model_type=m, fh=fh, verbose=False)\n",
    "            best_params[(prov, m)] = {\"order\": order, \"seasonal_order\": seas, \"score\": float(score)}\n",
    "            print(f\"best MASE={score:.3f}\")\n",
    "    print(\"‚úÖ Tuning done.\")\n",
    "    return best_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f8bf6ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6 ‚Äî Cache best params (with force_retune + sanity check)\n",
    "def load_or_tune_best_params(train_df, models_to_run=(\"arima\",\"sarima\"),\n",
    "                             cache_path=CACHE_PATH, fh=1, force_retune=False,\n",
    "                             min_success_ratio=0.6):\n",
    "    def _tune():\n",
    "        best = tune_all_provinces(train_df, models_to_run=models_to_run, fh=fh)\n",
    "        serial = {\"|\".join(k): v for k, v in best.items()}\n",
    "        with open(cache_path, \"w\") as f:\n",
    "            json.dump(serial, f, indent=2)\n",
    "        print(f\"‚úÖ Saved tuned best parameters to {cache_path}\")\n",
    "        return best\n",
    "\n",
    "    if force_retune or (not os.path.exists(cache_path)):\n",
    "        print(\"‚è≥ Running tuning from scratch...\")\n",
    "        return _tune()\n",
    "\n",
    "    print(f\"üîÅ Loading cached best parameters from {cache_path}\")\n",
    "    with open(cache_path, \"r\") as f:\n",
    "        best_params = json.load(f)\n",
    "    best_params = {tuple(k.split(\"|\")): v for k, v in best_params.items()}\n",
    "\n",
    "    # If cache is mostly bad (inf), re-tune automatically\n",
    "    scores = [v.get(\"score\", np.inf) for v in best_params.values()]\n",
    "    ok = np.isfinite(scores).sum() if len(scores) else 0\n",
    "    ratio = ok / max(len(scores), 1)\n",
    "    if ratio < min_success_ratio:\n",
    "        print(f\"‚ö†Ô∏è Cache quality low ({ok}/{len(scores)} finite). Retuning‚Ä¶\")\n",
    "        return _tune()\n",
    "\n",
    "    return best_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f2b528a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7 ‚Äî Holdout predictions & overlays\n",
    "\n",
    "def holdout_predictions(train_df, test_df, best_params, models_to_run=(\"arima\",\"sarima\")):\n",
    "    rows = []\n",
    "\n",
    "    # Baselines first\n",
    "    for prov, gtr in train_df.groupby(\"province\"):\n",
    "        gte = test_df[test_df[\"province\"] == prov]\n",
    "        if gte.empty: \n",
    "            continue\n",
    "        y_tr = gtr[\"y\"].astype(float).copy()\n",
    "        y_te = gte[\"y\"].astype(float).copy()\n",
    "\n",
    "        # Naive-1 (prev quarter)\n",
    "        y_all = pd.concat([y_tr, y_te])\n",
    "        yhat1 = y_all.shift(1).loc[y_te.index]\n",
    "        met1 = metrics(y_te, yhat1)\n",
    "        met1[\"MASE\"] = mase(y_te, yhat1, y_tr, season=1)\n",
    "        for q, yt, yp in zip(gte[\"quarter\"].values, y_te.values, yhat1.values):\n",
    "            rows.append({\"province\":prov, \"model\":\"naive1\", \"quarter\":pd.to_datetime(q), \"y_true\":yt, \"y_pred\":yp, **met1})\n",
    "\n",
    "        # Naive-4 (seasonal)\n",
    "        yhat4 = y_all.shift(SEASONAL_PERIOD).loc[y_te.index]\n",
    "        met4 = metrics(y_te, yhat4)\n",
    "        met4[\"MASE\"] = mase(y_te, yhat4, y_tr, season=SEASONAL_PERIOD)\n",
    "        for q, yt, yp in zip(gte[\"quarter\"].values, y_te.values, yhat4.values):\n",
    "            rows.append({\"province\":prov, \"model\":\"naive4\", \"quarter\":pd.to_datetime(q), \"y_true\":yt, \"y_pred\":yp, **met4})\n",
    "\n",
    "    # ARIMA / SARIMA (tuned)\n",
    "    for prov, gtr in train_df.groupby(\"province\"):\n",
    "        gte = test_df[test_df[\"province\"] == prov]\n",
    "        if gte.empty:\n",
    "            continue\n",
    "        y_tr = gtr[\"y\"].astype(float)\n",
    "        y_te = gte[\"y\"].astype(float)\n",
    "\n",
    "        for m in models_to_run:\n",
    "            params = best_params.get((prov, m))\n",
    "            if not params:\n",
    "                continue\n",
    "            order = tuple(params[\"order\"])\n",
    "            seas  = tuple(params[\"seasonal_order\"])\n",
    "            try:\n",
    "                res = fit_sarimax_endog(y_tr, order=order, seasonal_order=seas)\n",
    "                yhat = forecast_steps(res, steps=len(y_te)).values\n",
    "                met = metrics(y_te, yhat)\n",
    "                # Reasonable MASE baseline choices:\n",
    "                season_for_mase = 1 if m == \"arima\" else SEASONAL_PERIOD\n",
    "                met[\"MASE\"] = mase(y_te, pd.Series(yhat, index=gte.index), y_tr, season=season_for_mase)\n",
    "                for q, yt, yp in zip(gte[\"quarter\"].values, y_te.values, yhat):\n",
    "                    rows.append({\"province\":prov, \"model\":m, \"quarter\":pd.to_datetime(q), \"y_true\":yt, \"y_pred\":yp, **met})\n",
    "            except Exception as e:\n",
    "                # fallback (skip model for this province)\n",
    "                print(f\"‚ö†Ô∏è  {prov}-{m} failed on holdout with {e}\")\n",
    "\n",
    "    pred_df = pd.DataFrame(rows)\n",
    "    return pred_df\n",
    "\n",
    "def summarize_holdout(pred_df):\n",
    "    by_model = pred_df.groupby(\"model\")[[\"MAE\",\"RMSE\",\"sMAPE\",\"MASE\"]].mean().round(2).sort_values(\"MASE\")\n",
    "    print(\"=== Holdout averages across provinces ===\")\n",
    "    display(by_model)\n",
    "    by_pm = pred_df.groupby([\"province\",\"model\"])[[\"MAE\",\"RMSE\",\"sMAPE\",\"MASE\"]].mean().round(2)\n",
    "    return by_model, by_pm\n",
    "\n",
    "def plot_holdout_overlay(pred_df, models_to_plot=(\"naive1\",\"naive4\",\"arima\",\"sarima\")):\n",
    "    provs = sorted(pred_df[\"province\"].unique())\n",
    "    cols = 4\n",
    "    rows = math.ceil(len(provs) / cols)\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(4*cols, 2.6*rows), sharex=False, sharey=False)\n",
    "    axes = axes.ravel()\n",
    "\n",
    "    for i, prov in enumerate(provs):\n",
    "        ax = axes[i]\n",
    "        g = pred_df[pred_df[\"province\"] == prov]\n",
    "        truth = g.drop_duplicates(\"quarter\")[[\"quarter\",\"y_true\"]].sort_values(\"quarter\")\n",
    "        ax.plot(truth[\"quarter\"], truth[\"y_true\"], color=\"black\", linewidth=2.0, label=\"True\")\n",
    "\n",
    "        for m in models_to_plot:\n",
    "            gm = g[g[\"model\"] == m].sort_values(\"quarter\")\n",
    "            if gm.empty: \n",
    "                continue\n",
    "            ax.plot(gm[\"quarter\"], gm[\"y_pred\"], \"--\", linewidth=1.5, label=m.upper())\n",
    "\n",
    "        ax.set_title(prov.upper(), fontsize=10)\n",
    "        ax.tick_params(axis=\"x\", labelrotation=45)\n",
    "        ax.grid(True, linestyle=\"--\", alpha=0.3)\n",
    "\n",
    "    for j in range(i+1, rows*cols):\n",
    "        fig.delaxes(axes[j])\n",
    "    handles, labels = axes[0].get_legend_handles_labels()\n",
    "    fig.legend(handles, labels, loc=\"lower center\", ncol=min(len(models_to_plot)+1, 6), frameon=False, bbox_to_anchor=(0.5, -0.02))\n",
    "    fig.suptitle(\"True vs Predicted (Holdout) ‚Äî Na√Øve / ARIMA / SARIMA\", y=1.02, fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e3940270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8 ‚Äî Rolling forecast evolution (visual)\n",
    "\n",
    "def rolling_evolution_plot(df_all, best_params, model_name=\"sarima\", fh=1, initial=INITIAL_CV_START):\n",
    "    rows = []\n",
    "    for tr, te in rolling_split(df_all, initial=initial, fh=fh):\n",
    "        cutoff = tr[\"quarter\"].max()\n",
    "        for prov, gtr in tr.groupby(\"province\"):\n",
    "            gte = te[te[\"province\"] == prov]\n",
    "            if gte.empty:\n",
    "                continue\n",
    "            params = best_params.get((prov, model_name))\n",
    "            if not params:\n",
    "                continue\n",
    "            order = tuple(params[\"order\"])\n",
    "            seas  = tuple(params[\"seasonal_order\"])\n",
    "            y_tr = gtr[\"y\"].astype(float)\n",
    "            if y_tr.notna().sum() < MIN_TRAIN_PER_FOLD:\n",
    "                continue\n",
    "            try:\n",
    "                res = fit_sarimax_endog(y_tr, order=order, seasonal_order=seas)\n",
    "                yhat = forecast_steps(res, steps=len(gte)).values\n",
    "                rows.append(pd.DataFrame({\n",
    "                    \"cutoff\": cutoff,\n",
    "                    \"province\": prov,\n",
    "                    \"quarter\": gte[\"quarter\"].values,\n",
    "                    \"y_true\": gte[\"y\"].values,\n",
    "                    \"y_pred\": yhat\n",
    "                }))\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    if not rows:\n",
    "        print(\"No rolling predictions to plot.\")\n",
    "        return\n",
    "\n",
    "    df = pd.concat(rows, ignore_index=True)\n",
    "    df[\"quarter\"] = pd.to_datetime(df[\"quarter\"])\n",
    "    df[\"cutoff\"]  = pd.to_datetime(df[\"cutoff\"])\n",
    "    df = df.sort_values([\"province\",\"cutoff\",\"quarter\"])\n",
    "\n",
    "    cuts = sorted(df[\"cutoff\"].unique())\n",
    "    cut_rank = {c:i for i,c in enumerate(cuts)}\n",
    "    df[\"cut_rank\"] = df[\"cutoff\"].map(cut_rank)\n",
    "\n",
    "    provs = sorted(df[\"province\"].unique())\n",
    "    cols = 4\n",
    "    rows = math.ceil(len(provs)/cols)\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(4*cols, 2.6*rows), sharex=False, sharey=False)\n",
    "    axes = axes.ravel()\n",
    "\n",
    "    for i, prov in enumerate(provs):\n",
    "        ax = axes[i]\n",
    "        g = df[df[\"province\"] == prov]\n",
    "        truth = g.drop_duplicates(\"quarter\")[[\"quarter\",\"y_true\"]].sort_values(\"quarter\")\n",
    "        ax.plot(truth[\"quarter\"], truth[\"y_true\"], color=\"black\", linewidth=1.8, label=\"True\")\n",
    "        for c, gc in g.groupby(\"cutoff\"):\n",
    "            r = cut_rank[c]\n",
    "            alpha = 0.25 + 0.6 * (r / (len(cuts)-1 if len(cuts) > 1 else 1))\n",
    "            ax.plot(gc[\"quarter\"], gc[\"y_pred\"], \"--\", color=\"tab:orange\", alpha=alpha, linewidth=1.2)\n",
    "        latest = g[g[\"cutoff\"] == cuts[-1]].sort_values(\"quarter\")\n",
    "        if not latest.empty:\n",
    "            ax.plot(latest[\"quarter\"], latest[\"y_pred\"], \"--\", color=\"tab:orange\", linewidth=1.8, label=\"Latest\")\n",
    "        ax.set_title(prov.upper(), fontsize=10)\n",
    "        ax.tick_params(axis=\"x\", labelrotation=45)\n",
    "        ax.grid(True, linestyle=\"--\", alpha=0.3)\n",
    "\n",
    "    for j in range(i+1, rows*cols):\n",
    "        fig.delaxes(axes[j])\n",
    "\n",
    "    handles, labels = axes[0].get_legend_handles_labels()\n",
    "    fig.legend(handles[:2], labels[:2], loc=\"lower center\", ncol=2, frameon=False, bbox_to_anchor=(0.5, -0.02))\n",
    "    fig.suptitle(f\"Rolling forecast evolution ‚Äî {model_name.upper()}\", y=1.02, fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ee9fe094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 1990-03-31 ‚Üí 2018-12-31 | Test: 2019-03-31 ‚Üí 2025-06-30\n",
      "‚è≥ Running tuning from scratch...\n",
      "üßÆ Tuning 11 provinces √ó 2 models\n",
      "‚Üí [1/22] AB ‚Äî ARIMA ... best MASE=inf\n",
      "‚Üí [2/22] AB ‚Äî SARIMA ... best MASE=inf\n",
      "‚Üí [3/22] BC ‚Äî ARIMA ... best MASE=inf\n",
      "‚Üí [4/22] BC ‚Äî SARIMA ... best MASE=inf\n",
      "‚Üí [5/22] CAN ‚Äî ARIMA ... best MASE=inf\n",
      "‚Üí [6/22] CAN ‚Äî SARIMA ... best MASE=inf\n",
      "‚Üí [7/22] MB ‚Äî ARIMA ... best MASE=inf\n",
      "‚Üí [8/22] MB ‚Äî SARIMA ... best MASE=inf\n",
      "‚Üí [9/22] NB ‚Äî ARIMA ... best MASE=inf\n",
      "‚Üí [10/22] NB ‚Äî SARIMA ... best MASE=inf\n",
      "‚Üí [11/22] NL ‚Äî ARIMA ... best MASE=inf\n",
      "‚Üí [12/22] NL ‚Äî SARIMA ... best MASE=inf\n",
      "‚Üí [13/22] NS ‚Äî ARIMA ... best MASE=inf\n",
      "‚Üí [14/22] NS ‚Äî SARIMA ... best MASE=inf\n",
      "‚Üí [15/22] ON ‚Äî ARIMA ... best MASE=inf\n",
      "‚Üí [16/22] ON ‚Äî SARIMA ... best MASE=inf\n",
      "‚Üí [17/22] PE ‚Äî ARIMA ... best MASE=inf\n",
      "‚Üí [18/22] PE ‚Äî SARIMA ... best MASE=inf\n",
      "‚Üí [19/22] QC ‚Äî ARIMA ... best MASE=inf\n",
      "‚Üí [20/22] QC ‚Äî SARIMA ... best MASE=inf\n",
      "‚Üí [21/22] SK ‚Äî ARIMA ... best MASE=inf\n",
      "‚Üí [22/22] SK ‚Äî SARIMA ... best MASE=inf\n",
      "‚úÖ Tuning done.\n",
      "‚úÖ Saved tuned best parameters to best_params_arima_cache.json\n",
      "‚ÑπÔ∏è  Using default ARIMA for ab\n",
      "‚ÑπÔ∏è  Using default SARIMA for ab\n",
      "‚ÑπÔ∏è  Using default ARIMA for bc\n",
      "‚ÑπÔ∏è  Using default SARIMA for bc\n",
      "‚ÑπÔ∏è  Using default ARIMA for can\n",
      "‚ÑπÔ∏è  Using default SARIMA for can\n",
      "‚ÑπÔ∏è  Using default ARIMA for mb\n",
      "‚ÑπÔ∏è  Using default SARIMA for mb\n",
      "‚ÑπÔ∏è  Using default ARIMA for nb\n",
      "‚ÑπÔ∏è  Using default SARIMA for nb\n",
      "‚ÑπÔ∏è  Using default ARIMA for nl\n",
      "‚ÑπÔ∏è  Using default SARIMA for nl\n",
      "‚ÑπÔ∏è  Using default ARIMA for ns\n",
      "‚ÑπÔ∏è  Using default SARIMA for ns\n",
      "‚ÑπÔ∏è  Using default ARIMA for on\n",
      "‚ÑπÔ∏è  Using default SARIMA for on\n",
      "‚ÑπÔ∏è  Using default ARIMA for pe\n",
      "‚ÑπÔ∏è  Using default SARIMA for pe\n",
      "‚ÑπÔ∏è  Using default ARIMA for qc\n",
      "‚ÑπÔ∏è  Using default SARIMA for qc\n",
      "‚ÑπÔ∏è  Using default ARIMA for sk\n",
      "‚ÑπÔ∏è  Using default SARIMA for sk\n",
      "‚ö†Ô∏è  Dropping ('ab', 'arima') due to inf score during tuning.\n",
      "‚ö†Ô∏è  Dropping ('ab', 'sarima') due to inf score during tuning.\n",
      "‚ö†Ô∏è  Dropping ('bc', 'arima') due to inf score during tuning.\n",
      "‚ö†Ô∏è  Dropping ('bc', 'sarima') due to inf score during tuning.\n",
      "‚ö†Ô∏è  Dropping ('can', 'arima') due to inf score during tuning.\n",
      "‚ö†Ô∏è  Dropping ('can', 'sarima') due to inf score during tuning.\n",
      "‚ö†Ô∏è  Dropping ('mb', 'arima') due to inf score during tuning.\n",
      "‚ö†Ô∏è  Dropping ('mb', 'sarima') due to inf score during tuning.\n",
      "‚ö†Ô∏è  Dropping ('nb', 'arima') due to inf score during tuning.\n",
      "‚ö†Ô∏è  Dropping ('nb', 'sarima') due to inf score during tuning.\n",
      "‚ö†Ô∏è  Dropping ('nl', 'arima') due to inf score during tuning.\n",
      "‚ö†Ô∏è  Dropping ('nl', 'sarima') due to inf score during tuning.\n",
      "‚ö†Ô∏è  Dropping ('ns', 'arima') due to inf score during tuning.\n",
      "‚ö†Ô∏è  Dropping ('ns', 'sarima') due to inf score during tuning.\n",
      "‚ö†Ô∏è  Dropping ('on', 'arima') due to inf score during tuning.\n",
      "‚ö†Ô∏è  Dropping ('on', 'sarima') due to inf score during tuning.\n",
      "‚ö†Ô∏è  Dropping ('pe', 'arima') due to inf score during tuning.\n",
      "‚ö†Ô∏è  Dropping ('pe', 'sarima') due to inf score during tuning.\n",
      "‚ö†Ô∏è  Dropping ('qc', 'arima') due to inf score during tuning.\n",
      "‚ö†Ô∏è  Dropping ('qc', 'sarima') due to inf score during tuning.\n",
      "‚ö†Ô∏è  Dropping ('sk', 'arima') due to inf score during tuning.\n",
      "‚ö†Ô∏è  Dropping ('sk', 'sarima') due to inf score during tuning.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[55]\u001b[39m\u001b[32m, line 40\u001b[39m\n\u001b[32m     37\u001b[39m         \u001b[38;5;28;01mdel\u001b[39;00m best_params[k]\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# 4) Holdout predictions + summary\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m pred_holdout = \u001b[43mholdout_predictions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbest_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodels_to_run\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMODELS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m by_model, by_pm = summarize_holdout(pred_holdout)\n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m# 5) Overlays\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[53]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36mholdout_predictions\u001b[39m\u001b[34m(train_df, test_df, best_params, models_to_run)\u001b[39m\n\u001b[32m     15\u001b[39m y_all = pd.concat([y_tr, y_te])\n\u001b[32m     16\u001b[39m yhat1 = y_all.shift(\u001b[32m1\u001b[39m).loc[y_te.index]\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m met1 = \u001b[43mmetrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_te\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myhat1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m met1[\u001b[33m\"\u001b[39m\u001b[33mMASE\u001b[39m\u001b[33m\"\u001b[39m] = mase(y_te, yhat1, y_tr, season=\u001b[32m1\u001b[39m)\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m q, yt, yp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(gte[\u001b[33m\"\u001b[39m\u001b[33mquarter\u001b[39m\u001b[33m\"\u001b[39m].values, y_te.values, yhat1.values):\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[49]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36mmetrics\u001b[39m\u001b[34m(y_true, y_pred)\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmetrics\u001b[39m(y_true, y_pred):\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     mae  = \u001b[43mmean_absolute_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m     rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n\u001b[32m      6\u001b[39m     eps = \u001b[32m1e-8\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/erdos_ds_environment/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:218\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    214\u001b[39m         skip_parameter_validation=(\n\u001b[32m    215\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    216\u001b[39m         )\n\u001b[32m    217\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    224\u001b[39m     msg = re.sub(\n\u001b[32m    225\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    226\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    227\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    228\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/erdos_ds_environment/lib/python3.12/site-packages/sklearn/metrics/_regression.py:284\u001b[39m, in \u001b[36mmean_absolute_error\u001b[39m\u001b[34m(y_true, y_pred, sample_weight, multioutput)\u001b[39m\n\u001b[32m    228\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Mean absolute error regression loss.\u001b[39;00m\n\u001b[32m    229\u001b[39m \n\u001b[32m    230\u001b[39m \u001b[33;03mThe mean absolute error is a non-negative floating point value, where best value\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    279\u001b[39m \u001b[33;03m0.85...\u001b[39;00m\n\u001b[32m    280\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    281\u001b[39m xp, _ = get_namespace(y_true, y_pred, sample_weight, multioutput)\n\u001b[32m    283\u001b[39m _, y_true, y_pred, sample_weight, multioutput = (\n\u001b[32m--> \u001b[39m\u001b[32m284\u001b[39m     \u001b[43m_check_reg_targets_with_floating_dtype\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultioutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mxp\u001b[49m\n\u001b[32m    286\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    287\u001b[39m )\n\u001b[32m    289\u001b[39m output_errors = _average(\n\u001b[32m    290\u001b[39m     xp.abs(y_pred - y_true), weights=sample_weight, axis=\u001b[32m0\u001b[39m, xp=xp\n\u001b[32m    291\u001b[39m )\n\u001b[32m    292\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(multioutput, \u001b[38;5;28mstr\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/erdos_ds_environment/lib/python3.12/site-packages/sklearn/metrics/_regression.py:209\u001b[39m, in \u001b[36m_check_reg_targets_with_floating_dtype\u001b[39m\u001b[34m(y_true, y_pred, sample_weight, multioutput, xp)\u001b[39m\n\u001b[32m    160\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Ensures y_true, y_pred, and sample_weight correspond to same regression task.\u001b[39;00m\n\u001b[32m    161\u001b[39m \n\u001b[32m    162\u001b[39m \u001b[33;03mExtends `_check_reg_targets` by automatically selecting a suitable floating-point\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    205\u001b[39m \u001b[33;03m    correct keyword.\u001b[39;00m\n\u001b[32m    206\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    207\u001b[39m dtype_name = _find_matching_floating_dtype(y_true, y_pred, sample_weight, xp=xp)\n\u001b[32m--> \u001b[39m\u001b[32m209\u001b[39m y_type, y_true, y_pred, sample_weight, multioutput = \u001b[43m_check_reg_targets\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    210\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultioutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mxp\u001b[49m\n\u001b[32m    211\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    213\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m y_type, y_true, y_pred, sample_weight, multioutput\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/erdos_ds_environment/lib/python3.12/site-packages/sklearn/metrics/_regression.py:115\u001b[39m, in \u001b[36m_check_reg_targets\u001b[39m\u001b[34m(y_true, y_pred, sample_weight, multioutput, dtype, xp)\u001b[39m\n\u001b[32m    112\u001b[39m xp, _ = get_namespace(y_true, y_pred, multioutput, xp=xp)\n\u001b[32m    114\u001b[39m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m y_true = \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    116\u001b[39m y_pred = check_array(y_pred, ensure_2d=\u001b[38;5;28;01mFalse\u001b[39;00m, dtype=dtype)\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/erdos_ds_environment/lib/python3.12/site-packages/sklearn/utils/validation.py:1105\u001b[39m, in \u001b[36mcheck_array\u001b[39m\u001b[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[39m\n\u001b[32m   1099\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1100\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFound array with dim \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marray.ndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m,\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1101\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m while dim <= 2 is required\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1102\u001b[39m     )\n\u001b[32m   1104\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ensure_all_finite:\n\u001b[32m-> \u001b[39m\u001b[32m1105\u001b[39m     \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1106\u001b[39m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1107\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1108\u001b[39m \u001b[43m        \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1109\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mallow-nan\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1110\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1112\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[32m   1113\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[32m   1114\u001b[39m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/erdos_ds_environment/lib/python3.12/site-packages/sklearn/utils/validation.py:120\u001b[39m, in \u001b[36m_assert_all_finite\u001b[39m\u001b[34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[32m    118\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    124\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/erdos_ds_environment/lib/python3.12/site-packages/sklearn/utils/validation.py:169\u001b[39m, in \u001b[36m_assert_all_finite_element_wise\u001b[39m\u001b[34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[39m\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name == \u001b[33m\"\u001b[39m\u001b[33mX\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[32m    153\u001b[39m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[32m    154\u001b[39m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[32m    155\u001b[39m     msg_err += (\n\u001b[32m    156\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m does not accept missing values\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    157\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    167\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m#estimators-that-handle-nan-values\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    168\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m169\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[31mValueError\u001b[39m: Input contains NaN."
     ]
    }
   ],
   "source": [
    "# Cell 9 ‚Äî Run all\n",
    "\n",
    "# 1) Load & prep\n",
    "raw = pd.read_csv(DATA_PATH)\n",
    "uni_df = build_univariate_frame(raw)\n",
    "\n",
    "# 2) Holdout split\n",
    "train, test = chrono_split(uni_df, cutoff=CUTOFF_DATE)\n",
    "print(f\"Train: {train['quarter'].min().date()} ‚Üí {train['quarter'].max().date()} | \"\n",
    "      f\"Test: {test['quarter'].min().date()} ‚Üí {test['quarter'].max().date()}\")\n",
    "\n",
    "# 3) Tune or load cache\n",
    "MODELS = (\"arima\",\"sarima\")\n",
    "best_params = load_or_tune_best_params(train, MODELS, CACHE_PATH, fh=1, force_retune=True)\n",
    "\n",
    "DEFAULTS = {\n",
    "    \"arima\":  {\"order\": (1,1,1), \"seasonal_order\": (0,0,0,0)},\n",
    "    \"sarima\": {\"order\": (0,1,1), \"seasonal_order\": (0,1,1,4)},\n",
    "}\n",
    "\n",
    "provinces = sorted(train[\"province\"].unique())\n",
    "for prov in provinces:\n",
    "    for m in MODELS:\n",
    "        v = best_params.get((prov, m))\n",
    "        if (v is None) or (not np.isfinite(v.get(\"score\", np.inf))):\n",
    "            print(f\"‚ÑπÔ∏è  Using default {m.upper()} for {prov}\")\n",
    "            best_params[(prov, m)] = {\n",
    "                \"order\": DEFAULTS[m][\"order\"],\n",
    "                \"seasonal_order\": DEFAULTS[m][\"seasonal_order\"],\n",
    "                \"score\": float(\"nan\"),\n",
    "            }\n",
    "\n",
    "# (Optional) drop hopeless results (score==inf) to avoid holdout failures\n",
    "for k, v in list(best_params.items()):\n",
    "    if not np.isfinite(v.get(\"score\", np.inf)):\n",
    "        print(f\"‚ö†Ô∏è  Dropping {k} due to inf score during tuning.\")\n",
    "        del best_params[k]\n",
    "\n",
    "# 4) Holdout predictions + summary\n",
    "pred_holdout = holdout_predictions(train, test, best_params, models_to_run=MODELS)\n",
    "by_model, by_pm = summarize_holdout(pred_holdout)\n",
    "\n",
    "# 5) Overlays\n",
    "plot_holdout_overlay(pred_holdout, models_to_plot=(\"naive1\",\"naive4\",\"arima\",\"sarima\"))\n",
    "\n",
    "# 6) Rolling evolution (pick ARIMA or SARIMA)\n",
    "rolling_evolution_plot(uni_df, best_params, model_name=\"sarima\", fh=1, initial=INITIAL_CV_START)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb91a1a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8190c846",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "My Env Display Name",
   "language": "python",
   "name": "erdos_ds_environment"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
